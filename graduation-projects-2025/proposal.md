# 졸업 논문 계획서

## 1. 논문 제목 (가제)
강화학습 기반 다중 목표 경로 최적화를 위한 보상 함수 및 알고리즘 비교 연구

## 2. 연구 배경 및 필요성
자율주행 로봇은 물류, 농업, 서비스 로봇 등 다양한 분야에서 활용되고 있으며,
주행 효율성과 안정성 확보를 위해 경로 최적화(Path Optimization) 기술이 핵심 과제로 부상하고 있다.

기존의 전통적 경로 탐색 알고리즘(A*, Dijkstra 등)은 정적인 환경에서는 효율적이지만,
동적 환경이나 다중 목표(예: 시간, 에너지, 충돌 회피) 상황에서는 한계가 존재한다.

이에 본 연구는 강화학습(Reinforcement Learning) 을 적용하여 로봇이 환경과 상호작용하면서
스스로 주행 전략을 학습하고, 보상 함수 설계를 통해 다중 목표를 반영하는 최적의 경로 탐색 기법을 제안하고자 한다.

## 3. 연구 목표
자율 이동 로봇의 다중 목표 경로 최적화를 위해
보상 함수 설계와 강화학습 알고리즘(DQN, PPO, SAC + α) 을 적용·비교함으로써
가장 효율적이고 안정적인 주행 성능을 도출하는 것을 목표로 한다.

### 주요 목표
* ROS2 + Webots 기반의 시뮬레이션 환경 구축
* 보상 함수 설계를 통한 다중 목표(시간, 에너지, 충돌 회피) 반영
* DQN, PPO, SAC + α 알고리즘 비교 및 성능 평가
* 최적 알고리즘 및 보상 구조 도출

## 4. 선행 연구 분석
연구자	                    연구 내용	                            한계점
박광석 외 (2019)	          DQN 기반 로봇 이동 경로 최적화 연구	      단일 목표(시간) 중심, 에너지 고려 부족
선우영민 외 (2021)	        심층 강화학습을 이용한 장애물 회피 주행	    충돌 회피 중심, 효율적 경로 학습 부족
Schulman et al. (2017)	  PPO 알고리즘 제안, 안정적 학습 가능	      연산량 증가, 실시간 적용 어려움
Haarnoja et al. (2018)	  SAC 알고리즘으로 탐색성과 안정성 향상	    보상 설계 복잡, 환경 종속적
-> 본 연구는 기존 연구의 한계(단일 목표·환경 종속성)를 보완하여,
다중 목표 보상 구조와 다양한 RL 알고리즘을 비교함으로써 실용적 최적화 방향을 제시한다.

## 5. 연구 내용 및 방법
1) 시뮬레이션 환경 구축
* ROS2 Humble + Webots 연동
* TurtleBot3 Burger 모델 사용
* 이동 목표와 장애물이 포함된 실험 맵 설계

2) 상태(State) 및 행동(Action) 정의
* 상태: 로봇 위치, 거리 센서, 목표 거리, 속도
* 행동: 선속도(linear.x), 회전속도(angular.z)

3) 보상 함수 설계
* 목표 도달 시: +10
* 충돌 시: -10
* 거리 감소 시: +0.1 × Δdistance
* 이동 시간 증가 시: -0.01
* 에너지 소비 반영: 속도 급변 시 감점

4) 강화학습 알고리즘 적용
* DQN: Q-value 기반 오프폴리시 학습
* PPO: 안정적 업데이트를 통한 학습 효율 개선
* SAC: 탐색성(Exploration) 강화

5) 성능 평가 지표
* 목표 도달 성공률 (%)
* 평균 이동 시간 (s)
* 평균 충돌 횟수
* 평균 총 보상
* 학습 수렴 속도

## 6. 예상 결과 및 기대 효과
- 결과 예측
* PPO와 SAC 알고리즘이 DQN보다 빠른 수렴성과 안정적 학습 결과를 보일 것으로 예상
* 보상 함수 설계에 따라 각 알고리즘의 학습 경향 차이 확인 가능

- 기대 효과
* 자율주행 로봇의 효율적 경로 탐색 기술 향상
* 다중 목표 기반 보상 구조 설계를 통한 실환경 적용 가능성 확보
* ROS2 + Webots 기반 강화학습 시뮬레이션 플랫폼 구축 사례 제공

## 7. 연구 추진 일정
| 단계      | 내용                  | 기간        |
|---|---|---|
| 1단계 | 선행 연구 조사          | 2025.10.10~2025.10.31 |
| 2단계 | 데이터 수집 및 전처리   | 2025.11.01~2025.11.30 |
| 3단계 | 모델 개발 및 실험     | 2025.12.01~2026.01.31 |
| 4단계 | 결과 분석 및 논문 작성  | 2026.02.01~2026.02.28 |
