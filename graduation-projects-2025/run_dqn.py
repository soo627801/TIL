import torch
import numpy as np
import rclpy
from rclpy.node import Node
from geometry_msgs.msg import Twist
from sensor_msgs.msg import LaserScan
from nav_msgs.msg import Odometry
from tb3_env import TurtleBot3Env
from train_dqn import DQN

# -------------------------------
# ë¡œë´‡ ììœ¨ì£¼í–‰ ì‹¤í–‰ (í•™ìŠµëœ ëª¨ë¸ ì‚¬ìš©)
# -------------------------------

class DQNRunner(Node):
    def __init__(self):
        super().__init__('dqn_runner')

        # í™˜ê²½ ì„¤ì •
        self.env = TurtleBot3Env()
        state_dim = self.env.observation_space.shape[0]
        action_dim = 3

        # í•™ìŠµëœ ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°
        self.model = DQN(state_dim, action_dim)
        model_path = "/home/soo/ros2_ws/src/tb3_rl/tb3_rl/trained_dqn.pth"
        self.model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))
        self.model.eval()

        self.get_logger().info("âœ… Trained DQN model loaded successfully!")

        # ì‹¤í–‰ ë£¨í”„ íƒ€ì´ë¨¸
        self.timer = self.create_timer(0.1, self.run_step)
        self.state = self.env.reset()

    def run_step(self):
        with torch.no_grad():
            q_values = self.model(torch.FloatTensor(self.state))
            action_idx = torch.argmax(q_values).item()

        if action_idx == 0:
            action = [-0.5, 0.1]   # ì™¼ìª½ íšŒì „
        elif action_idx == 1:
            action = [0.0, 0.15]   # ì§ì§„
        else:
            action = [0.5, 0.1]    # ì˜¤ë¥¸ìª½ íšŒì „

        next_state, reward, done, _ = self.env.step(action)
        self.state = next_state

        if done:
            self.get_logger().info("ğŸ Episode finished. Resetting environment.")
            self.state = self.env.reset()


def main(args=None):
    rclpy.init(args=args)
    node = DQNRunner()
    try:
        rclpy.spin(node)
    except KeyboardInterrupt:
        pass
    finally:
        node.env.close()
        node.destroy_node()
        rclpy.shutdown()


if __name__ == '__main__':
    main()

